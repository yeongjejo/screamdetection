# scream_panns.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchaudio
import math

from pathlib import Path
import random

import sounddevice as sd
import numpy as np

from datetime import datetime
import socket
import json

# ğŸ”¹ PANNs Cnn14 ê°€ì ¸ì˜¤ê¸° (panns_transfer ë ˆí¬ ê¸°ì¤€)
#   - ë ˆí¬ êµ¬ì¡°ì— ë”°ë¼ import ê²½ë¡œëŠ” ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
from panns_transfer_to_gtzan.pytorch.models import Cnn14   # ì˜ˆ: panns_transfer/models.py ì•ˆì— ìˆìŒ


# =========================
#  Dataset (waveform ì¶œë ¥)
# =========================
class ScreamWaveformDataset(Dataset):
    """
    root_dir ì•ˆì—
      - scream/ *.wav   (label=1)
      - non_scream/ *.wav (label=0)
    êµ¬ì¡°ë¡œ ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    PANNs ìª½ì—ì„œ log-melì„ ê³„ì‚°í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” waveformë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

    âœ… ë³€ê²½ì :
    - í•œ íŒŒì¼ì´ 2ì´ˆë³´ë‹¤ ê¸¸ë©´, 2ì´ˆ ë‹¨ìœ„ë¡œ ì—¬ëŸ¬ chunkë¡œ ë‚˜ëˆ ì„œ
      ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ëŠ˜ë¦¼.
      ì˜ˆ) 5ì´ˆì§œë¦¬ â†’ 2ì´ˆì”© 3ê°œ(chunk)ë¡œ ì‚¬ìš© (ë§ˆì§€ë§‰ì€ padding)
    """

    def __init__(self, root_dir,
                 sample_rate=32000,  # PANNs ê¸°ë³¸ 32kHz
                 duration=2.0,
                 is_train=True):
        self.root_dir = Path(root_dir)
        self.sample_rate = sample_rate
        self.num_samples = int(sample_rate * duration)
        self.is_train = is_train

        # (path, chunk_idx, label) í˜•íƒœë¡œ ì €ì¥
        self.chunks = []

        for label_name, label in [('non_scream', 0), ('scream', 1)]:
            class_dir = self.root_dir / label_name
            if not class_dir.exists():
                continue

            for wav_path in class_dir.rglob('*.wav'):
                # íŒŒì¼ ê¸¸ì´ í™•ì¸ ìœ„í•´ í•œ ë²ˆ ë¡œë“œ (dataset ìƒì„± ì‹œ 1íšŒ)
                wav, sr = torchaudio.load(str(wav_path))

                # mono
                if wav.shape[0] > 1:
                    wav = wav.mean(dim=0, keepdim=True)

                # resample to target sr
                if sr != self.sample_rate:
                    wav = torchaudio.functional.resample(wav, sr, self.sample_rate)

                total_len = wav.shape[1]
                if total_len <= 0:
                    continue  # ë¹ˆ íŒŒì¼ ë°©ì§€

                # ëª‡ ê°œì˜ 2ì´ˆ chunkë¥¼ ë§Œë“¤ì§€ ê²°ì •
                num_chunks = max(1, math.ceil(total_len / self.num_samples))

                for chunk_idx in range(num_chunks):
                    self.chunks.append((wav_path, chunk_idx, label))

        print(f"[{root_dir}] ì´ íŒŒì¼ ê¸°ë°˜ chunk ìˆ˜: {len(self.chunks)}")

    def __len__(self):
        return len(self.chunks)

    def _load_audio(self, path, chunk_idx):
        """
        ì§€ì •ëœ chunk_idxì— í•´ë‹¹í•˜ëŠ” 2ì´ˆì§œë¦¬ êµ¬ê°„ë§Œ ì˜ë¼ì„œ ë°˜í™˜
        """
        wav, sr = torchaudio.load(str(path))  # (C, T)

        # mono
        if wav.shape[0] > 1:
            wav = wav.mean(dim=0, keepdim=True)

        # resample to 32k
        if sr != self.sample_rate:
            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)

        total_len = wav.shape[1]

        # í•´ë‹¹ chunkì˜ ì‹œì‘/ë ìƒ˜í”Œ ì¸ë±ìŠ¤
        start = chunk_idx * self.num_samples
        end = start + self.num_samples

        # startê°€ íŒŒì¼ ê¸¸ì´ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë³´í˜¸
        if start >= total_len:
            # ì•ˆì „ì¥ì¹˜: ë§ˆì§€ë§‰ ë¶€ë¶„ìœ¼ë¡œ ê°•ì œ ì´ë™
            start = max(0, total_len - self.num_samples)
            end = start + self.num_samples

        # ë§ˆì§€ë§‰ chunkëŠ” paddingì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        if end > total_len:
            pad_len = end - total_len
            wav = F.pad(wav, (0, pad_len))
            total_len = wav.shape[1]

        wav = wav[:, start:end]  # (1, num_samples)

        return wav

    def _augment(self, wav):
        # ê°„ë‹¨í•œ ì¦ê°•: gain + ì¡ìŒ
        if random.random() < 0.5:
            wav = wav * (0.5 + random.random())  # 0.5~1.5ë°°
        if random.random() < 0.5:
            noise = torch.randn_like(wav) * 0.003
            wav = wav + noise
        return wav

    def __getitem__(self, idx):
        path, chunk_idx, label = self.chunks[idx]

        wav = self._load_audio(path, chunk_idx)

        if self.is_train:
            wav = self._augment(wav)

        # PANNs êµ¬í˜„ì— ë”°ë¼ (B, T) ë˜ëŠ” (B, 1, T)ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” (T,) í˜•íƒœë¥¼ ë°˜í™˜í•˜ê³ , collate í›„ (B, T)ë¡œ ì‚¬ìš©í•  ì˜ˆì •
        wav = wav.squeeze(0)  # (T,)

        return wav, torch.tensor(label, dtype=torch.float32)


# =========================
#  PANNs ê¸°ë°˜ ë¹„ëª… íƒì§€ ëª¨ë¸
# =========================
class PANNsScreamModel(nn.Module):
    """
    PANNs Cnn14 backbone + binary classifier head
    """

    def __init__(self,
                 sample_rate=32000,
                 window_size=1024,
                 hop_size=320,
                 mel_bins=64,
                 fmin=50,
                 fmax=14000,
                 classes_num=527,
                 pretrained_checkpoint: str = None,
                 freeze_backbone: bool = True):
        super().__init__()

        # Cnn14 backbone
        self.backbone = Cnn14(
            sample_rate=sample_rate,
            window_size=window_size,
            hop_size=hop_size,
            mel_bins=mel_bins,
            fmin=fmin,
            fmax=fmax,
            classes_num=classes_num,
        )

        # ì‚¬ì „í•™ìŠµ weight ë¡œë“œ (AudioSet)
        if pretrained_checkpoint is not None:
            ckpt = torch.load(pretrained_checkpoint, map_location='cpu')
            # ë ˆí¬ì—ì„œ ì œê³µí•˜ëŠ” í‚¤ ì´ë¦„ì— ë§ê²Œ ì¡°ì • í•„ìš”
            state_dict = ckpt.get('model', ckpt)
            self.backbone.load_state_dict(state_dict, strict=False)
            print(f"Loaded pretrained weights from: {pretrained_checkpoint}")

        # backbone freeze (ì›í•˜ë©´ í’€ì–´ì„œ end-to-end fine-tune)
        if freeze_backbone:
            print('++++')
            for p in self.backbone.parameters():
                p.requires_grad = False

            print('----')

        # Cnn14ì˜ ì„ë² ë”© ì°¨ì›ì€ 2048 (ë ˆí¬ ê¸°ì¤€)
        self.head = nn.Sequential(
            nn.Linear(2048, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 1)  # binary classification (scream / non_scream)
        )

    def forward(self, x):
        """
        x: waveform tensor, shape = (B, T)
           (í•„ìš”ì‹œ (B, 1, T)ì—ì„œ squeeze í•´ì„œ ì‚¬ìš©)
        """
        if x.dim() == 3:
            # (B, 1, T) -> (B, T)
            x = x.squeeze(1)

        # PANNs Cnn14ì˜ forwardëŠ” (waveform, mixup_lambda=None) í˜•íƒœì¸ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
        # ì‹¤ì œ êµ¬í˜„ì— ë”°ë¼ ì¸ì í˜•ì‹ì€ ì¡°ì • í•„ìš”í•©ë‹ˆë‹¤.
        out_dict = self.backbone(x, None)
        embedding = out_dict['embedding']  # (B, 2048)

        logit = self.head(embedding).squeeze(1)  # (B,)
        return logit


# =========================
#  Train / Eval ë£¨í”„
# =========================
def train_one_epoch(model, loader, optimizer, device):
    model.train()
    bce = nn.BCEWithLogitsLoss()

    total_loss = 0.0
    total_correct = 0
    total = 0

    for wav, y in loader:
        # wav: (B, T)
        wav = wav.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        logits = model(wav)
        loss = bce(logits, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * wav.size(0)
        preds = (torch.sigmoid(logits) > 0.5).float()
        total_correct += (preds == y).sum().item()
        total += wav.size(0)

        print('train loss : ', (total_loss / total), end='\r')

    return total_loss / total, total_correct / total


@torch.no_grad()
def eval_one_epoch(model, loader, device):
    model.eval()
    bce = nn.BCEWithLogitsLoss()

    total_loss = 0.0
    total_correct = 0
    total = 0

    for wav, y in loader:
        wav = wav.to(device)
        y = y.to(device)

        logits = model(wav)
        loss = bce(logits, y)

        total_loss += loss.item() * wav.size(0)
        preds = (torch.sigmoid(logits) > 0.5).float()
        total_correct += (preds == y).sum().item()
        total += wav.size(0)

        print('valid loss : ', total_loss / total, end='\r')

    return total_loss / total, total_correct / total


# =========================
#  ë‹¨ì¼ íŒŒì¼ ì¶”ë¡ 
# =========================
@torch.no_grad()
def detect_scream_panns(model,
                        wav_path,
                        device,
                        sample_rate=32000,
                        duration=2.0):
    model.eval()
    wav_path = Path(wav_path)

    num_samples = int(sample_rate * duration)

    wav, sr = torchaudio.load(str(wav_path))  # (C, T)
    if wav.shape[0] > 1:
        wav = wav.mean(dim=0, keepdim=True)

    if sr != sample_rate:
        wav = torchaudio.functional.resample(wav, sr, sample_rate)

    if wav.shape[1] < num_samples:
        pad = num_samples - wav.shape[1]
        left = pad // 2
        right = pad - left
        wav = F.pad(wav, (left, right))
    elif wav.shape[1] > num_samples:
        start = (wav.shape[1] - num_samples) // 2
        wav = wav[:, start:start + num_samples]

    wav = wav.squeeze(0).unsqueeze(0).to(device)  # (1, T)

    logit = model(wav)
    prob = torch.sigmoid(logit).item()
    is_scream = prob >= 0.5

    return is_scream, prob


# =========================
#  main: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
# =========================
def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("Using device:", device)

    print(1)
    train_ds = ScreamWaveformDataset('./data/train', is_train=True)
    print(2)
    val_ds   = ScreamWaveformDataset('./data/val',   is_train=False)
    print(3)

    train_loader = DataLoader(train_ds, batch_size=32,
                              shuffle=True, num_workers=4)
    val_loader   = DataLoader(val_ds, batch_size=32,
                              shuffle=False, num_workers=4)

    # ğŸ”¹ PANNs ê¸°ë°˜ ëª¨ë¸ ìƒì„±
    model = PANNsScreamModel(
        pretrained_checkpoint='./Cnn14.pth',  # ì‹¤ì œ ê²½ë¡œ/íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •
        freeze_backbone=True
    ).to(device)


    # backboneì„ freezeí–ˆë‹¤ë©´ headë§Œ í•™ìŠµë˜ë¯€ë¡œ í•™ìŠµ ì†ë„/ì•ˆì •ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤.
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,
                                        model.parameters()),
                                 lr=1e-3)

    best_val_acc = 0.0
    num_epochs = 500

    for epoch in range(1, num_epochs + 1):
        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)
        val_loss, val_acc = eval_one_epoch(model, val_loader, device)

        print(f'Epoch {epoch:02d} | '
              f'train_loss={train_loss:.4f} acc={train_acc:.3f} | '
              f'val_loss={val_loss:.4f} acc={val_acc:.3f}')

        # if val_acc > best_val_acc:
        if True:
            torch.save(model.state_dict(), f'check_point/panns_scream_best_{epoch}.pt')
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                print(f'  >> Best model saved (val_acc={best_val_acc:.3f})')

    print("Training finished. Best val_acc:", best_val_acc)


@torch.no_grad()
def run_realtime_scream_detection_sliding(
    # checkpoint_path='panns_scream_best.pt',
    checkpoint_path='panns_scream_best - ë³µì‚¬ë³¸.pt',
    sample_rate=32000,
    window_duration=2.0,   # ëª¨ë¸ì´ ë³´ëŠ” ê¸¸ì´(2ì´ˆ)
    hop_duration=0.5,      # íŒì • ì£¼ê¸°(0.5ì´ˆ)
    threshold=0.7,
    device_str=None,
):
    """
    2ì´ˆì§œë¦¬ ë¶„ì„ ìœˆë„ìš°ë¥¼ 0.5ì´ˆë§ˆë‹¤ êµ´ë¦¬ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì‹¤ì‹œê°„ ë¹„ëª… ê°ì§€.

    - checkpoint_path : í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ê°€ì¤‘ì¹˜(panns_scream_best.pt)
    - sample_rate     : 32000 (í•™ìŠµ ì‹œì™€ ë™ì¼)
    - window_duration : í•œ ë²ˆì— ëª¨ë¸ì´ ë³´ëŠ” ê¸¸ì´ (2ì´ˆ)
    - hop_duration    : íŒì • ê°„ê²© (0.5ì´ˆ â†’ latency)
    - threshold       : ë¹„ëª…ì´ë¼ê³  íŒë‹¨í•  ê¸°ì¤€ í™•ë¥ 
    """

    UDP_IP = "127.0.0.1"  # ë°›ëŠ” ìª½ IP
    UDP_PORT = 2301  # ë°›ëŠ” ìª½ í¬íŠ¸

    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    # ë””ë°”ì´ìŠ¤ ì„ íƒ
    if device_str is None:
        device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device_str)
    print(f"[Realtime-Sliding] Using device: {device}")

    # 1) ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    model = PANNsScreamModel(
        sample_rate=sample_rate,
        pretrained_checkpoint=None,   # ì²´í¬í¬ì¸íŠ¸ì— ì´ë¯¸ backbone+head ì €ì¥ëœ ìƒíƒœ
        freeze_backbone=True,
    )
    state_dict = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()

    # 2) ìœˆë„ìš° / hop ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    window_samples = int(sample_rate * window_duration)   # 2ì´ˆ â†’ 64000 (32kHz ê¸°ì¤€)
    hop_samples    = int(sample_rate * hop_duration)      # 0.5ì´ˆ â†’ 16000

    # print(f"[Realtime-Sliding] window={window_duration:.2f}s ({window_samples} samples), "
    #       f"hop={hop_duration:.2f}s ({hop_samples} samples)")
    # print("  ë§ˆì´í¬ ì…ë ¥ì„ 0.5ì´ˆë§ˆë‹¤ ë°›ì•„ì„œ, í•­ìƒ ìµœê·¼ 2ì´ˆë¥¼ ëª¨ë¸ì— ë„£ì–´ ê°ì§€í•©ë‹ˆë‹¤.")
    # print("  ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C ë¥¼ ëˆ„ë¥´ì„¸ìš”.\n")

    # 3) ìµœê·¼ 2ì´ˆ ë²„í¼ ì´ˆê¸°í™” (ì²˜ìŒì—ëŠ” 0ìœ¼ë¡œ ì±„ì›€)
    buffer = torch.zeros(window_samples, device=device)  # (T,)
    t1 = None
    detection = False

    try:
        while True:
            # 4) ë§ˆì´í¬ì—ì„œ 0.5ì´ˆ ë¶„ëŸ‰ ë…¹ìŒ
            print("ğŸ§ Listening ...", end="\r")
            audio = sd.rec(
                frames=hop_samples,
                samplerate=sample_rate,
                channels=1,
                dtype='float32',
            )
            sd.wait()  # 0.5ì´ˆ ëŒ€ê¸°

            # audio: (hop_samples, 1) -> (hop_samples,)
            new_block = audio.squeeze(1)     # numpy (hop_samples,)
            new_block = torch.from_numpy(new_block).to(device)  # (hop_samples,)

            # ë§Œì•½ ë…¹ìŒ ê¸¸ì´ê°€ ë¶€ì¡±í•˜ë©´ íŒ¨ë”©
            if new_block.numel() < hop_samples:
                pad_len = hop_samples - new_block.numel()
                new_block = F.pad(new_block, (0, pad_len))

            # 5) ë²„í¼ë¥¼ ì™¼ìª½ìœ¼ë¡œ hopë§Œí¼ ë°€ê³ , ë’¤ì— ìƒˆ ë¸”ë¡ ë¶™ì´ê¸°
            buffer = torch.cat([buffer[hop_samples:], new_block], dim=0)  # ì—¬ì „íˆ (window_samples,)



            # 6) í˜„ì¬ ë²„í¼(ìµœê·¼ 2ì´ˆ)ë¥¼ ëª¨ë¸ì— ì…ë ¥
            wav = buffer.unsqueeze(0)  # (1, T)
            logit = model(wav)
            prob = torch.sigmoid(logit).item()
            is_scream = prob >= threshold

            now = datetime.now()

            if detection == False and (t1 is None or abs((now - t1).total_seconds()) >= 5):
                detection = True
                print('íƒì§€ì¤‘...')

            if is_scream and detection:
                data = {}
                t1 = now
                detection = False
                time_str = now.strftime("%Y-%m-%d %H:%M:%S.%f").split(' ')

                data['detect_type'] = 0
                data['detect_date'] = time_str[0]
                data['detect_time'] = time_str[1]
                data['detect_zone'] = ''

                # JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™” í›„ ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©
                message = json.dumps(data).encode("utf-8")

                # print(message)

                sock.sendto(message, (UDP_IP, UDP_PORT))
                print("ì „ì†¡ ì™„ë£Œ")


            print(prob)

            # status = "ğŸš¨ SCREAM DETECTED" if is_scream else "â€¦ normal"
            # print(f"\rProb={prob:.3f}  =>  {status}           ", end="")

    except KeyboardInterrupt:
        print("\n[Realtime-Sliding] ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[Realtime-Sliding] ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # í•™ìŠµì‹œ
    # main()

    # ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ì‹œ
    run_realtime_scream_detection_sliding(
        checkpoint_path='./check_point/panns_scream_best_1.pt',
        sample_rate=32000,
        window_duration=2.0,
        hop_duration=0.5,
        threshold=0.7,
    )